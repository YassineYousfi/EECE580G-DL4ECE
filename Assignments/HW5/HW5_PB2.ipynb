{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW5_PB2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dPFybf1EPpLQ"},"source":["# Problem 2. Semantic Segmentation\n","\n","First: Switch to GPU.\n","\n","In this problem you will train a CNN to perform Semantic Segmentation of selfies. The goal is to produce a binary mask where 0 is 'background' and 1 is 'person'.\n","\n","This could be useful if you are interested in writing your own Instagram Stories filter :)\n","\n","We will use a dataset originating from here. https://github.com/clovaai/ext_portrait_segmentation\n","\n","Most of the code provided comes from the CamVid example. With a few minor changes to adapt to a new dataset.\n","\n","1. Use colab file explorer to get a sense of how the data is organized. Complete the data manipulation code\n","2. Train a U-Net with efficientnet-B0 as a back-bone\n","- Using decoder_block_type='upsampling'\n","- Using `sm.losses.CategoricalFocalLoss()`\n","- Adam default LR \n","- 10 epochs \n","- Show accuracy\n","- Use model checkpointing to save and recover the best weights (w.r.t. validation accuracy)\n","- You can get inspiration from the notebook `segmentation_camvid.ipynb`\n","- You should get a validation accuracy ~96% or more\n","- 10 epochs might take around 4 minutes\n","\n","3. Repeat 2. with decoder_block_type='transpose'\n","\n","4. Compare and comment."]},{"cell_type":"markdown","metadata":{"id":"PQWNkmWglcSf"},"source":["## Imports and data"]},{"cell_type":"code","metadata":{"id":"KWeYX0oxJKbG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605017834882,"user_tz":300,"elapsed":12807,"user":{"displayName":"Yassine Yousfi","photoUrl":"","userId":"06317191099946336363"}},"outputId":"e2f9b8e0-2d79-4a4f-8112-7d743e997fc2"},"source":["import tensorflow as tf\n","import pathlib\n","import os\n","import glob\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL\n","from functools import partial\n","from tqdm import tqdm_notebook as tqdm\n","!pip install -U --quiet git+https://github.com/qubvel/segmentation_models\n","!pip install -U --quiet git+https://github.com/albumentations-team/albumentations\n","%env SM_FRAMEWORK=tf.keras\n","import segmentation_models as sm\n","import albumentations as A"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Building wheel for segmentation-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","env: SM_FRAMEWORK=tf.keras\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZE6_yskdy0eN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605017854250,"user_tz":300,"elapsed":2480,"user":{"displayName":"Yassine Yousfi","photoUrl":"","userId":"06317191099946336363"}},"outputId":"59607686-df08-4b71-93ad-78f2d5a0fe1e"},"source":["url =\"https://drive.google.com/uc?id=1-1xrQ2OzXZpnpxnQYJZVIUWs0Hixbr15\"\n","!gdown {url}\n","!unzip -q data_PB2.zip\n","!rm data_PB2.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-1xrQ2OzXZpnpxnQYJZVIUWs0Hixbr15\n","To: /content/data_PB2.zip\n","107MB [00:00, 124MB/s] \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24cEpOHMzIOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605017869926,"user_tz":300,"elapsed":491,"user":{"displayName":"Yassine Yousfi","photoUrl":"","userId":"06317191099946336363"}},"outputId":"be414bc3-40d7-4429-91cb-e8fe8ba4130f"},"source":["data_dir = '/content/images_data_crop/'\n","data_dir = pathlib.Path(data_dir)\n","images_list = (data_dir).glob('*.jpg')\n","images_list = [str(x) for x in images_list]\n","np.random.shuffle(images_list)\n","valid_images_idx = np.loadtxt('val.txt',dtype=np.int64)\n","valid_images_list = np.array(images_list)[valid_images_idx]\n","train_images_list = set(images_list).difference(valid_images_list)\n","train_images_list = list(train_images_list)\n","valid_images_list = list(valid_images_list)\n","print(len(train_images_list), 'Training images')\n","print(len(valid_images_list), 'validation images')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1330 Training images\n","300 validation images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bu0D1vUtBWHn"},"source":["def get_mask_path(path):\n","  # YOUR CODE HERE\n","  return \n","\n","idx = 10\n","img = PIL.Image.open(train_images_list[idx])\n","plt.imshow(img)\n","img = PIL.Image.open(get_mask_path(train_images_list[idx]))\n","plt.imshow(img, cmap='jet', alpha=0.4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFj_daskPNqM"},"source":["NCLASSES = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBubWVXtDOzi"},"source":["train_masks_list = [get_mask_path(x) for x in train_images_list]\n","valid_masls_list = [get_mask_path(x) for x in valid_images_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCUvCFJk0q4D"},"source":["data_train = tf.data.Dataset.from_tensor_slices((train_images_list, train_masks_list))\n","data_val = tf.data.Dataset.from_tensor_slices((valid_images_list, valid_masls_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOpoVI_qlJLN"},"source":["BATCH_SIZE = 8\n","IMG_SIZE = (224, 192)\n","transforms_train = A.Compose([\n","            A.Resize(IMG_SIZE[0], IMG_SIZE[1], p=1),\n","            A.HorizontalFlip(p=0.5),              \n","            A.RandomSizedCrop(min_max_height=(IMG_SIZE[0]//2, IMG_SIZE[0]), height=IMG_SIZE[0], width=IMG_SIZE[1],\n","                             w2h_ratio=720/960, p=0.5),\n","            A.GaussNoise(p=0.1),\n","            A.MotionBlur(blur_limit=3, p=0.4),\n","            A.OneOf([A.RandomBrightnessContrast(), A.HueSaturationValue()], p=0.7),\n","            A.CLAHE(p=0.5)\n","        ])\n","\n","transforms_val = A.Compose([\n","            A.Resize(IMG_SIZE[0], IMG_SIZE[1], p=1)\n","        ])\n","\n","BACKBONE = 'efficientnetb0'\n","preprocess_input = sm.get_preprocessing(BACKBONE)\n","\n","def aug_fn(image, mask, train):\n","    data = {\"image\": image, \"mask\": mask}\n","    if train:\n","      data = transforms_train(**data)\n","    else:\n","      data = transforms_val(**data)\n","    aug_img = data[\"image\"]\n","    aug_img = preprocess_input(aug_img)\n","    return aug_img, data[\"mask\"]\n","\n","def parse(image_path, mask_path):\n","    image = tf.io.read_file(image_path)\n","    image = tf.io.decode_png(image, channels=3)\n","    mask = tf.io.read_file(mask_path)\n","    mask = tf.io.decode_png(mask, channels=1) \n","    mask = tf.clip_by_value(mask, clip_value_min=0, clip_value_max=1) #only two classes 0 and 1\n","    return image, mask\n","\n","def process_data_train(image, mask):\n","    aug_img, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, mask, True], \n","                                Tout=[tf.float32, tf.uint8])\n","    aug_img.set_shape(IMG_SIZE+(3,))\n","    aug_mask = tf.squeeze(aug_mask)\n","    aug_mask = tf.one_hot(aug_mask,depth=NCLASSES)\n","    aug_mask.set_shape(IMG_SIZE+(NCLASSES,))\n","    return aug_img, aug_mask\n","\n","def process_data_val(image, mask):\n","    aug_img, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, mask, False], \n","                                Tout=[tf.float32, tf.uint8])\n","    aug_img.set_shape(IMG_SIZE+(3,))\n","    aug_mask = tf.squeeze(aug_mask)\n","    aug_mask = tf.one_hot(aug_mask,depth=NCLASSES)\n","    aug_mask.set_shape(IMG_SIZE+(NCLASSES,))\n","    return aug_img, aug_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCU08n6qlG_N"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","data_train = data_train.shuffle(buffer_size=BATCH_SIZE*4).map(parse, \n","                  num_parallel_calls=AUTOTUNE).cache().map(process_data_train,\n","                  num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n","data_val = data_val.map(parse, \n","                  num_parallel_calls=AUTOTUNE).cache().map(process_data_val,\n","                  num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA9CjyUscQ1d"},"source":["def view_image_batch(ds, model=None):\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    image, mask = next(iter(ds)) # extract 1 batch from the dataset\n","    if model is None:\n","      mask = tf.argmax(mask,axis=-1)\n","    else:\n","      mask = tf.argmax(model(image),axis=-1)\n","    image = np.clip((image.numpy()*std)+mean,0,1) # Doing the efn.preprocess_input inverse \n","    fig = plt.figure(figsize=(22, 10))\n","    for i in range(8):\n","        ax = fig.add_subplot(2, 4, i+1, xticks=[], yticks=[])\n","        ax.imshow(image[i])\n","        ax.imshow(mask[i], cmap='jet', alpha=0.4)\n","\n","view_image_batch(data_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPL68ocWDUXf"},"source":["## U-Net"]},{"cell_type":"code","metadata":{"id":"ubd3B4Eh7JBr"},"source":["IMG_SHAPE = IMG_SIZE + (3,)\n","# YOUR CODE HERE\n","# model = \n","# Fit and checkpoint \n","# Load best checkpoint after training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbjA-fN7Vq5P"},"source":["print('PREDICTED')\n","view_image_batch(data_val, model)\n","plt.show()\n","print('GROUND TRUTH')\n","view_image_batch(data_val)"],"execution_count":null,"outputs":[]}]}