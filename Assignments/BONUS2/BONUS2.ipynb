{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BONUS2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JK5gWTbLOAwv"},"source":["# Bonus Problem - Theory of Deep Learning - Implicit Regularization\n","\n","In this problem we will verify experimentally some recent theoretical results on implicit regularization in deep learning. \n","\n","Optional reading: http://www.offconvex.org/2019/07/10/trajectories-linear-nets/\n","\n","Many works have pointed out that Gradient Descent in high dimension and non convex problems seen in Deep Learning, converges to low ranks solutions.\n","\n","Implicit regularization means that Gradient Descent will converge to those low rank solutions without any explicit regularizer.\n","\n","In particular, we will look at **Linear Networks**, MLPs without any activation function in the hidden layers, we have discussed that in the lectures that activation functions should always be applied, otherwise layers without activation functions will collapse to a single hidden layer. But in reality there is more to it, and Linear Networks are central in recent theory of Deep Learning.\n","\n","In this problem we will try Linear Networks with different depths and examine the end-to-end transformation. We will show that the deeper the network, the smaller the effective dimension of the representation.\n","\n","We will use the MNIST dataset. Use a GPU Runtime.\n","\n","1- Write a function `get_model(num_hidden_layers)` that generates the following network architecture:\n","\n","- Flatten the input\n","- `num_hidden_layers` successive fully connected layers with size 128 for all of them - without any activation function - without bias\n","- An output layer of size 10\n","\n","2- Complete the function `compile_fit_get_rank(model, optimizer)`\n","\n","- compile\n","- fit for 5 epochs with default batch size \n","- Take all weight matrices using `model.trainable_variables` **besides the output matrix**, you should end up with a list of size `num_hidden_layer`\n","- Multiply the matrices: $W=W_1 W_2 W_3 ... W_{\\text{num_hidden}}$\n","- Compute the SVD of $W$ `tf.linalg.svd(W, compute_uv=False)`\n","- Return the singular values of $W$\n","\n","3- Run the following\n","```python\n","model = get_model(num_hidden_layers)\n","S = compile_fit_get_rank(model, optimizer)\n","```\n","for `num_hidden_layers` in [2,4,6,8] with Adam optimizer with default parameters\n","\n","4- Plot the singular values and their histograms (code provided) and **comment**\n","\n","5- Repeat with SGD optimizer default learning rate, without momentum and with momentum = 0.95 and **comment**"]},{"cell_type":"code","metadata":{"id":"qORz_96_OCMv"},"source":["import tensorflow as tf\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","import random\n","\n","def seed_everything(seed):\n","  os.environ['PYTHONHASHSEED']=str(seed)\n","  os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  tf.random.set_seed(seed)\n","\n","seed_everything(2020)\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","\n","def get_model(num_hidden_layers):\n","  # YOUR CODE HERE\n","  # \n","  # model = \n","  for i in range(num_hidden_layers):\n","    name='hidden_layer_'+str(i)\n","    # model.add \n","  name = 'output_layer'\n","  # model.add \n","  return model\n","\n","def compile_fit_get_rank(model, optimizer):\n","  # YOUR CODE HERE\n","  # loss = \n","  # model.compile \n","  # model.fit\n","  # get the matrices from model.trainable_variables (don't include the last layer's matrix)\n","  # multiply all of them \n","  # compute SVD\n","  # S = \n","  return S"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sh5EhUQHU0ZX"},"source":["optimizer = tf.keras.optimizers.Adam()\n","S = dict()\n","for num_hidden_layers in tqdm([2,4,6,8]):\n","  model = get_model(num_hidden_layers)\n","  S[num_hidden_layers] = compile_fit_get_rank(model, optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-Q3ZQgLVo04"},"source":["for num_hidden_layers in [2,4,6,8]:\n","  plt.plot(S[num_hidden_layers], label=str(num_hidden_layers))\n","plt.ylim([0,5])\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FspiyFak-fA"},"source":["for num_hidden_layers in [2,4,6,8]:\n","  plt.hist(S[num_hidden_layers][S[num_hidden_layers]<5], alpha=0.5, label=str(num_hidden_layers))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]}]}